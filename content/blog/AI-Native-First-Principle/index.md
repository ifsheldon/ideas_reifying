+++
title = "Draft: AI 原生和第一性原理"
description = "太长不看：多模态智能是关键"
draft = false

weight = 3

[taxonomies]
tags = ["AI", "增强现实"]

[extra]
feature_image = "kokichi_muta.png"
feature = true
+++

## 略微跑题

如果你被暗黑风的封面图吓到了，我很抱歉。这是我 2023 年看的《咒术回战》里的一个角色——与幸吉。

![Kokichi Muta](./kokichi_muta.png)
> 机械丸或者它的操控人——与幸吉

他的身世生平不重要，跟本文相关的是他的诅咒和天赋。他的诅咒是脆弱的身躯，但是“上帝给人关了一扇门，就会给人打开一扇窗”，他的天赋是“机械操纵”和超强的智谋。

当 GPT-4 横空出世并且我在日常中、研究中、工作中跟它打交道越来越多，我对它的印象从模糊变清晰，直到我想起了上面这张图，还有机械丸和与幸吉。如果你恰好对 GPT-4 特别熟悉，并且对《咒术回战》也有所了解，你没准会会心一笑。
但是两部分人的交集毕竟还是少数，所以我在这里稍微跑题了一下。这篇博客的主要内容很多地方会和这个人物有暗合之处。不过，我只是有种冲动，想把他也放进这篇博客里，所以你要是不了解这个人物，也不会影响理解。

> 如果你想再了解一下与幸吉，可以看看 [这里](https://zh.moegirl.org.cn/zh-hans/究极机械丸/与幸吉) 或者 [这里](https://jujutsu-kaisen.fandom.com/wiki/Kokichi_Muta)。

> 如果你恰好也是《咒术回战》的粉丝，那这里还有一个我发现的有意思的思想实验：如果把与幸吉当做 GPT-4，那他的结局和动机会不会是 AGI 的结局和动机呢？
>
> 我不知道答案，没准你有有意思的想法。

## 简写对照

文章里为了简洁，用了一些“常见”的英语缩写。“常见的英语缩写”可能不是对所有人都常见，所以下面是对照表。

| 英文缩写 |              英文全称               |   中文   |
|:----:|:-------------------------------:|:------:|
|  AI  |     Artificial Intelligence     |  人工智能  |
| AGI  | Artificial General Intelligence | 通用人工智能 |
|  CV  |         Computer Vision         | 计算机视觉  |
| NLP  |   Natural Language Processing   | 自然语言处理 |
| LLM  |      Large Language Model       | 大语言模型  |

## 问题和似是而非的回答

我常常看到很多人问下面这些问题或者类似的：

* 怎么样才算 AI 原生？
* 模型层和应用层的边界在哪里？
    * 另一个比较直白的翻译是：OpenAI 的边界在哪？

然后我之前想的一个博客选题是：为什么 Rabbit R1 不是 AI 原生的？

当我最近把这些问题和选题放在一起的时候，我突然发现，这些问题和它们延伸出来的问题，其实都在问同一个：

**“ AI 的第一性原理是什么？”**

当我们知道了 AI 的第一性原理，那剩下的问题就好解决了：

* 遵循 AI 第一性原理的应用就是 AI 原生的。AI 第一性原理直接延伸出来的能力就是 AI 的原生能力，也就是模型层的边界。
* 不能从 AI 第一性原理推导出来的能力就应该是应用层需要实现的能力，也就是 OpenAI 不会触及的区域。
* 同理，AI 原生的硬件是遵循 AI 第一性原理的硬件。而 Rabbit R1 是旧思路下的 AI 硬件，它不是 AI 原生的。

> **什么是第一性原理？**
>
> 似乎每个人都知道第一性原理是什么，好像在义务教育阶段就学过。但是好像没人能说清楚第一性原理的定义。 我也不知道（他们在说什么），所以以下是我自己的理解和定义：
>
> 第一性原理就是，顺着逻辑逆推或者拆解，把复杂的逻辑或者组合追溯到少数的基本部件。这个追溯的过程要不断审视各种东西的必要性，然后把不必要的或者衍生的东西去掉，直到剩下的东西是不可再分的。
> 这些不可再分的基本部件就是第一性原理。或者我们可以用一阶逻辑里的 “公理” 来近似。我们应该在义务教育里教一阶逻辑，而不是研究生课程🐶
>
> 至于怎么样才算“不可再分”或者怎么样才算“公（认的道）理”？That's a question!
>

## AI 第一性原理

那什么是 AI 的第一性原理呢？我认为是“信息量”，这不仅仅指信息的体量，还有信息的质量，还有信息在传递转换中的保真率。

熟悉 NLP 和 CV 等深度学习领域的小伙伴应该已经对“信息量”这个概念比较熟悉了，但是我还是想复述一下前人实践出来的相关经验：

1. 质量相同的情况下，数据的体量越多越好 —— 信息的体量越多越好
    * LLM 已经用上了全互联网的语料来训练。
    * SegmentAnything 用数据暴力，优雅地提高了语义分割（Segmentation）的表现。
2. 数据的体量相同的情况下，数据的质量要尽可能提高 —— 信息的质量越高越好
    * Phi-2 等表现出众的小模型说明语料的质量也很重要。
    * Stable Diffusion 的各种精调模型也是例子。
3. 在数据的表征（Representation）和模型的设计上，要尽可能减少人为的设计（heuristics）—— 信息的流动应该减少人为设计，让模型自己学习，尽可能保留**对模型有用的**信息
    * 已经有无数例子证明了这一点，两个经典例子是 AlexNet 起始的深度 CV 和基于深度学习的 NLP。
    * 新圣经 *The Bitter Lessons* 其实本质上也是说要尽量减少人为的设计。
    * 在现在这个深度学习时代，这个原则已经太显然了。可能很多没有接触过基于统计学的机器学习，直接从深度学习入门的学生，都会觉得本应如此。

对于不熟悉深度学习的小伙伴，第一第二条经验比较容易理解，跟人一样，读书多了会变聪明，但是读书的时候也要沙里淘金，筛选有价值、有意义的知识。
至于第三条经验，后面我会有比较多的例子来辅助说明。现在可以先打个比方。人的学习、成长历程常常是要自己选的。虽然有老师、父母的影响，但是最终对世界、人生的理解还是靠自己。当然，也有例子是父母、老师的干预和设计过多的。
想象一下有一个人，对于每一个事物、概念的理解都是完全依照 TA 的父母的。可能 TA 会活得好好的，但是我们不会觉得这个人很聪明。过度人为设计的模型就像是这样的一个人。

> **你说了 AI 第一性原理是什么，但是是怎么推导出来的呢？你说了是什么，但是没说为什么。**
>
> 确实！因为 AI 这个领域太广了，理论很多，噪声、狂热也很多，所以很难从很泛的概念或者问题逆推。很泛的概念或者问题类似于 “Sora 遵循了 AI 第一性原理吗？” “AI 第一性原理跟 AGI 有什么关系？”。
>
> 我对 AI 第一性原理的理解是从我遇上的各种实际的例子逆推来的，所以后面我会详细介绍这些例子。在介绍这些例子的时候，我会尝试分析它们底层的问题。
> Hopefully, 当我把这些例子和底层的问题讲完之后，你可以看到他们之间的共性。这些共性，再往前推一步，就靠近 AI 第一性原理了。
>
> 这样，你只需要相信我的例子，不需要盲从我的推理，自己从例子出发，相信你自己的推理，就可以达到同样的结论。

### AI 第一性原理对系统设计的意义

以上已经简单解释了 AI 第一性原理的概念，但是它对系统设计的意义是什么？这里的“系统”我指的是 AI 模型、包含 AI 模型的更大系统。我会举几个例子，并且说说我自己的思考。这些例子不是按时间顺序排序的。

第一个例子是 Andrej Karpathy 前几天发布的教程 *Let's build the GPT Tokenizer*。关注 AI 特别是 LLM 的人应该都看了 *Let's build the GPT Tokenizer*，吧？如果没有看，强烈建议去看一下！Andrej 是个传奇，他的教程也非常有意思而且易懂。

在教程里，他说到 LLM 有时候会有奇怪的表现，具体是这十一个问题：

* 为什么 LLM 不能拼写单词？Why can't LLM spell words?
* 为什么 LLM 不能做简单的字符串处理，例如逆转字符串？Why can't LLM do super simple string processing tasks like reversing a string?
* 为什么 LLM 的对除了英语之外的语言的表现更差？Why is LLM worse at non-English languages (e.g. Japanese)?
* 为什么 LLM 在简单算术上都做不好？Why is LLM bad at simple arithmetic?
* 为什么 GPT2 编程 Python 的时候会有很多不必要的麻烦？Why did GPT2 have more than necessary trouble coding in Python?
* 为什么大模型看见“<|endoftext|>”的时候就突然中断了？Why did my LLM abruptly halt when it sees the string "<|endoftext|>"?
* 这个奇怪的警告“末尾带有空格”是什么？What is this weird warning I get about a "trailing whitespace"?
* 我问 LLM 关于“SolidGoldMagikarp”，为什么它突然绷不住了？Why the LLM break if l ask it about "SolidGoldMagikarp"?
* 为什么用大模型的时候我应该用 YAML 而不是 JSON？Why should I prefer to use YAML over JSON with LLMs?
* 为什么 LLM 其实不算是在做端到端语言建模？Why is LLM not actually end-to-end language modeling?
* 上述这些痛苦的根源是什么？What is the real root of suffering?

这些问题追根溯源，是分词这个步骤（Tokenization）和分词器（Tokenizer）引起的。而分词器是 LLM 的设计里非常重要的一项*人为*设计。分词器把语言里的词分成更小的部分或者整合成更大的部分，这些都叫词元（Token），例如： “word” 可能会分割成 “wo” 和 “rd”两个词元，而“早啊，吃了吗”可能会因为出现频率比较高而被整合成一个词元。这些词元，才是
LLM 书写的基本单位。
中文等语言，分词器做的会更加复杂了。细节不赘述了，可以看 Andrej 的教程。

> Andrej 不是说分词器是训练出来的吗？为什么说它是人为设计？
>
> “训练”这个词已经有点滥用了，其实分词器的训练只能算迭代。“迭代”和“训练”的区别在这里就不深究了，简单地说就是：分词器的迭代不涉及到模型参数的更新，而“训练”这个词本应该是指不断更新模型参数这个过程的。
>
> 而且假如说你仔细看了教程的话，在教程一开始，Andrej 就说了分词器的训练是语料预处理的一部分；而且在教程后半部分，他列举了很多“训练”分词器的时候人为引入的规则。这些都说明分词器是人为的设计（heuristics）。

Andrej 举了一个很有意思的例子，是一些奇怪的复合词，比如说“SolidGoldMagikarp”。这个词没有什么明显的含义，但是分词器把它当成了一整个词元，可能是某个 Reddit 用户的用户名。迭代分词器的时候，可能语料里有很多这个用户的发言，所以它被整合成了一个词元。
大模型在处理词元的时候，会把词元转换成词嵌入（embedding），然后在训练的时候不断更新每个词元的词嵌入和大模型自己的参数。

> **词嵌入是什么？**
>
> 语言模型并不能直接处理词元，它们处理的是词元对应的词嵌入。一个词嵌入就是一串数字，比如说“你”这个词元可能对于的词嵌入是 [0.1, 0.2, 0.15, ......]，这串数字的长度是人为设定的，一般来说，词嵌入的长度以千计。
>
> 大模型在训练过程中也会不断更新词嵌入，这些词嵌入相当于是大模型对某个词元的理解，虽然人不能解释这串数字的含义。 每个词元的词嵌入一开始是随机数，只有模型开始学习了之后，词嵌入不断更新，才最终变成了大模型对某个词元的理解。
>
> 整个流程的大意如下图，上半部分是流程示意，下半部分是例子。最后的双箭头指的是：前向，词嵌入输入模型，然后也可以反向更新词嵌入。
>
> ![tokenization_embedding](./tokenization_embedding.png)

但是，可能大模型训练用的语料跟迭代分词器用的语料不一样。这样的话，大模型虽然有“SolidGoldMagikarp”的词嵌入，但是它训练的时候从来没有见过这个词，所以这个词的词嵌入从来没有更新过，一直是一开始的随机数。
大模型不能理解这些词嵌入（随机数），在理解的盲区里，所以遇上这些词之后就出现奇怪的行为了。

在这个例子里，我们仔细审视，至少有三个地方是（或者可以）人为设定的：

1. 什么字符串作为一个词元？
    * 例如我们可以人为添加 “<\|endoftext\|>” 这个字符串作为一个词元
2. 分词器的设置、迭代数据和迭代策略
    * Andrej 提到的一个例子是：Sentencepiece 迭代分词器的时候可以设置把罕见词替换成 “\<unk\>” 这个词元，“unk” 是 “unknown” 的缩写，意思是未知字符。
    * 至于迭代数据，假如说我们迭代分词器用的数据就不包含中文，那么注定这个分词器对中文的处理不好。
    * 另外，分词器怎么处理数字，会直接影响到 LLM 的数学能力。
3. 用于 LLM 训练的数据：一些用来跑分的英语大模型，从一开始训练的时候就去掉了非英语的数据，这样虽然可以节省成本，但是注定模型不会理解英语之外的其他语言，即使它本身足够智能。

> **思考题一：大模型训练里的歧视？**
>
> 如果一个产品的说明书只有英文，如果说明书作者说：我们只考虑英语用户，你会不会觉得被歧视了？
> 同样的，当你在 LLM 的训练报告或者论文里看到类似于“我们剔除了数据集/语料中的非英文部分”，你会不会觉得被歧视了？
>
> 还有更微妙的，假如分词器迭代语料里有99%是英文，然后分词器迭代设置是：开启罕见词过滤、常见词覆盖率是 99%，意味着出现少于 1% 的词都会被替换为 “\<unk\>”，你会不会觉得被歧视了？
>
> **思考题二：AGI 到来了吗？**

这个例子能拓展的话题太多了，但是我们先回归正题，也就是这个例子跟 AI 第一性原理有什么关系。 LLM 的例子里，数据的体量和质量的重要性不需要我多说了，我们需要联想的是第三条，也就是
> 信息的流动应该减少人为设计，让模型自己学习，尽可能保留对模型有用的信息

假如说我们能够去掉分词器，每一个字节就是一个词元，那么 Andrej 提到的十一个问题里的大部分就可以迎刃而解了。这些问题之所以存在，还是在于分词这个步骤，以及这个步骤里人工设计的部分（heuristics）。我可以说这甚至影响到了 AGI 的到来。

在这个例子里，系统指的是分词迭代数据、分词器、LLM 训练数据和 LLM 的组合。我们从表象上，也就是 Andrej 提到的十一个问题，追溯到了工程上的根源，也就是分词器。但是思维上追根溯源，本质在哪里？我觉得还是人为的设计。人类觉得对世界的理解是正确而且有效的，而且很多时候我们觉得世界是可以理解的，是有秩序的，所以很多时候会把我们的思维融入到我们的创造里。
但是其实我们都不完全了解我们自己的思维，更别说完全理解世界了。甚至就连围棋，我们对它的理解，可能都没有从零训练、不看棋谱的 AlphaZero 要深刻。而且其实我们的思维很大部分是“混乱” （见《快思慢想》，写得很好）。

虽然算不上无数，但是很多例子已经告诉我们，理解世界的方式有很多。而 Rich Sutton 写的《The Bitter Lesson （苦涩的教训）》也总结了类似的结论，也就是 AI 理解世界的方式不一定要符合人类的设计，我们需要减少人为的设计才可能训练出更强大的 AI。

> **但是等一下，我们能不能设计出完美的分词器来解决这些问题呢？**
>
> 这个问题其实跟 NLP 历史上的一个问题类似，就是“我们能够用规则来建模、理解语言吗”。而语言是我们思维的一种体现，其实问题也可以转换成“我们能够用规则来建模、理解我们的思维吗”。
> 我的答案是“小部分可以，例如数学，但是大部分是不能用规则来框定、理解的”，那翻译过来就是“在一些特定应用上，你可以试着设计完美的分词器，但是通用的完美分词器是不存在的”。
>

TODO: LLM 语音对话例子

### AI 第一性原理对产品设计的意义

TODOs:

* 模型即产品
* 产品驱动的模型开发
* Rabbit R1
* 梯度 API 和 AI OS

## 总结和亮点

TODO

## 后记

TODO

## 元数据

版本：0.0.2

日期：2024-02-19

版权协议：[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)

### 更新日志

2024.02.25: 写完了 AI 第一性原理的基本概念和 Andrej 的 *Let's build the GPT Tokenizer* 的例子