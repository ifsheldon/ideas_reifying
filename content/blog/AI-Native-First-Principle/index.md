+++
title = "Draft: AI 原生和第一性原理"
description = "太长不看：AI 第一性原理是信息量；遵循 AI 第一性的是 AI 原生；多模态智能是关键"
draft = false

weight = 3

[taxonomies]
tags = ["AI", "增强现实"]

[extra]
feature_image = "kokichi_muta.png"
feature = true
+++

## 略微跑题

如果你被暗黑风的封面图吓到了，我很抱歉。这是我 2023 年看的《咒术回战》里的一个角色——与幸吉。

![Kokichi Muta](./kokichi_muta.png)
> 机械丸或者它的操控人——与幸吉

他的身世生平不重要，跟本文相关的是他的诅咒和天赋。他的诅咒是脆弱的身躯，但是“上帝给人关了一扇门，就会给人打开一扇窗”，他的天赋是“机械操纵”和超强的智谋。

当 GPT-4 横空出世并且我在日常中、研究中、工作中跟它打交道越来越多，我对它的印象从模糊变清晰，直到我想起了上面这张图，还有机械丸和与幸吉。如果你恰好对 GPT-4 特别熟悉，并且对《咒术回战》也有所了解，你没准会会心一笑。
但是两部分人的交集毕竟还是少数，所以我在这里稍微跑题了一下。这篇博客的主要内容很多地方会和这个人物有暗合之处。不过，我只是有种冲动，想把他也放进这篇博客里，所以你要是不了解这个人物，也不会影响理解。

> 如果你想再了解一下与幸吉，可以看看 [这里](https://zh.moegirl.org.cn/zh-hans/究极机械丸/与幸吉) 或者 [这里](https://jujutsu-kaisen.fandom.com/wiki/Kokichi_Muta)。

> 如果你恰好也是《咒术回战》的粉丝，那这里还有一个我发现的有意思的思想实验：如果把与幸吉当做 GPT-4，那他的结局和动机会不会是 AGI 的结局和动机呢？
>
> 我不知道答案，没准你有有意思的想法。

## 简写对照

文章里为了简洁，用了一些“常见”的英语缩写。“常见的英语缩写”可能不是对所有人都常见，所以下面是对照表。

| 英文缩写 |              英文全称               |    中文    |
|:----:|:-------------------------------:|:--------:|
|  AI  |     Artificial Intelligence     |   人工智能   |
| AGI  | Artificial General Intelligence |  通用人工智能  |
| ASR  |  Automatic Speech Recognition   |  自动语音识别  |
|  CV  |         Computer Vision         |  计算机视觉   |
| LLM  |      Large Language Model       |  大语言模型   |
| NLP  |   Natural Language Processing   |  自然语言处理  |
|  SD  |        Stable Diffusion         | SD 文生图模型 |
| TTS  |         Text to Speech          |  文字转语音   |

## 问题和似是而非的回答

我常常看到很多人问下面这些问题或者类似的：

* 怎么样才算 AI 原生？
* 模型层和应用层的边界在哪里？
    * 另一个比较直白的翻译是：OpenAI 的边界在哪？

然后我之前想的一个博客选题是：为什么 Rabbit R1 不是 AI 原生的？

当我最近把这些问题和选题放在一起的时候，我突然发现，这些问题和它们延伸出来的问题，其实都在问同一个：

**“ AI 的第一性原理是什么？”**

当我们知道了 AI 的第一性原理，那剩下的问题就好解决了：

* 遵循 AI 第一性原理的应用就是 AI 原生的。AI 第一性原理直接延伸出来的能力就是 AI 的原生能力，也就是模型层的边界。
* 不能从 AI 第一性原理直接推导出来的能力就应该是应用层需要实现的能力，也就是 OpenAI 不会触及的区域。
* 同理，AI 原生的硬件是贴合 AI 第一性原理的硬件。而 Rabbit R1 的设计不贴合AI 第一性原理，是旧思路下的 AI 硬件，它不是 AI 原生的。

> **什么是第一性原理？**
>
> 似乎每个人都知道第一性原理是什么，好像在义务教育阶段就学过。但是好像没人能说清楚第一性原理的定义。 我也不知道（他们在说什么），以下是我自己的理解和定义：
>
> 第一性原理就是，顺着逻辑逆推或者拆解，把复杂的逻辑或者组合追溯到少数的基本部件。这个追溯的过程要不断审视各种东西的必要性，然后把不必要的或者衍生的东西去掉，直到剩下的东西是不可再分的。
> 这些不可再分的基本部件就是第一性原理。或者我们可以用一阶逻辑里的 “公理” 来近似。我们应该在义务教育里教一阶逻辑，而不是研究生课程🐶
>
> 至于怎么样才算“不可再分”或者怎么样才算“公（认的道）理”？That's a question!
>

## AI 第一性原理

那什么是 AI 的第一性原理呢？我认为是“信息量”，这不仅仅指信息的体量，还有信息的质量，还有信息在传递转换中的保真率。

熟悉 NLP 和 CV 等深度学习领域的小伙伴应该已经对“信息量”这个概念比较熟悉了，但是我还是想复述一下前人实践出来的相关经验：

1. 质量相同的情况下，数据的体量越多越好 —— 信息的体量越多越好
    * LLM 已经用上了全互联网的语料来训练。
    * SegmentAnything 用数据暴力，优雅地提高了语义分割（Segmentation）的表现。
2. 数据的体量相同的情况下，数据的质量要尽可能提高 —— 信息的质量越高越好
    * Phi-2 等表现出众的小模型说明语料的质量也很重要。
    * Stable Diffusion 的各种精调模型也是例子。
3. 在数据的表征（Representation）和模型的设计上，要尽可能减少人为的设计（heuristics）—— 信息的流动应该减少人为设计，让模型自己学习，尽可能保留**对模型有用的**信息
    * 已经有无数例子证明了这一点，两个经典例子是 AlexNet 起始的深度 CV 和基于深度学习的 NLP。
    * 新圣经《The Bitter Lessons》其实本质上也是说要尽量减少人为的设计。
    * 在现在这个深度学习时代，这个原则已经太显然了。可能很多没有接触过基于统计学的机器学习，直接从深度学习入门的学生，都会觉得本应如此。

对于不熟悉深度学习的小伙伴，第一第二条经验比较容易理解，跟人一样，读书多了会变聪明，但是读书的时候也要沙里淘金，筛选有价值、有意义的知识。
至于第三条经验，后面我会有比较多的例子来辅助说明。现在可以先打个比方。人的学习、成长历程常常是要自己选的。虽然有老师、父母的影响，但是最终对世界、人生的理解还是靠自己。当然，也有例子是父母、老师的干预和设计过多的。
想象一下有一个人，对于每一个事物、概念的理解都是完全依照 TA 的父母的。可能 TA 会活得好好的，但是我们不会觉得这个人很聪明。过度人为设计的模型就像是这样的一个人。

> **你说了 AI 第一性原理是什么，但是是怎么推导出来的呢？你说了是什么，但是没说为什么。**
>
> 确实！因为 AI 这个领域太广了，理论很多，噪声、狂热也很多，所以很难从很泛的概念或者问题逆推。很泛的概念或者问题类似于 “Sora 遵循了 AI 第一性原理吗？” “AI 第一性原理跟 AGI 有什么关系？”。
>
> 我对 AI 第一性原理的理解是从我遇上的各种实际的例子逆推来的，所以后面我会详细介绍这些例子。在介绍这些例子的时候，我会尝试分析它们底层的问题。
> Hopefully, 当我把这些例子和底层的问题讲完之后，你可以看到他们之间的共性。这些共性，再往前推一步，就靠近 AI 第一性原理了。
>
> 这样，你只需要相信我的例子，不需要盲从我的推理，自己从例子出发，相信你自己的推理，就可以达到同样的结论。

### AI 第一性原理对系统设计的意义

以上已经简单解释了 AI 第一性原理的概念，但是它对系统设计的意义是什么？这里的“系统”我指的是 AI 模型、包含 AI 模型的更大系统。我会举几个例子，并且说说我自己的思考。这些例子不是按时间顺序排序的。

第一个例子是 Andrej Karpathy 前几天发布的教程 *Let's build the GPT Tokenizer*。关注 AI 特别是 LLM 的人应该都看了 *Let's build the GPT Tokenizer*，吧？如果没有看，强烈建议去看一下！Andrej 是个传奇，他的教程也非常有意思而且易懂。

在教程里，他说到 LLM 有时候会有奇怪的表现，具体是这十一个问题：

* 为什么 LLM 不能拼写单词？
* 为什么 LLM 不能做简单的字符串处理，例如逆转字符串？
* 为什么 LLM 的对除了英语之外的语言的表现更差？
* 为什么 LLM 在简单算术上都做不好？
* 为什么 GPT2 编程 Python 的时候会有很多不必要的麻烦？
* 为什么大模型看见“<|endoftext|>”的时候就突然中断了？
* 这个奇怪的警告“末尾带有空格”是什么？
* 我问 LLM 关于“SolidGoldMagikarp”，为什么它突然绷不住了？
* 为什么用大模型的时候我应该用 YAML 而不是 JSON？
* 为什么 LLM 其实不算是在做端到端语言建模？
* 上述这些痛苦的根源是什么？

这些问题追根溯源，是分词这个步骤（Tokenization）和分词器（Tokenizer）引起的。而分词器是 LLM 的设计里非常重要的一项*人为*设计。分词器把语言里的词分成更小的部分或者整合成更大的部分，这些都叫词元（Token），例如： “word” 可能会分割成 “wo” 和 “rd”两个词元，而“早啊，吃了吗”可能会因为出现频率比较高而被整合成一个词元。这些词元，才是
LLM 书写的基本单位。
中文等语言，分词器做的会更加复杂了。细节不赘述了，可以看 Andrej 的教程。

> **Andrej 不是说分词器是训练出来的吗？为什么说它是人为设计？**
>
> “训练”这个词已经有点滥用了，其实分词器的训练只能算迭代。“迭代”和“训练”的区别在这里就不深究了，简单地说就是：分词器的迭代不涉及到模型参数的更新，而“训练”这个词本应该是指不断更新模型参数这个过程的。
>
> 而且假如说你仔细看了教程的话，在教程一开始，Andrej 就说了分词器的训练是语料预处理的一部分；而且在教程后半部分，他列举了很多“训练”分词器的时候人为引入的规则。这些都说明分词器是人为的设计（heuristics）。

Andrej 举了一个很有意思的例子，是一些奇怪的复合词，比如说“SolidGoldMagikarp”。这个词没有什么明显的含义，但是分词器把它当成了一整个词元，可能是某个 Reddit 用户的用户名。迭代分词器的时候，可能语料里有很多这个用户的发言，所以它被整合成了一个词元。
大模型在处理词元的时候，会把词元转换成词嵌入（embedding），然后在训练的时候不断更新每个词元的词嵌入和大模型自己的参数。

> **词嵌入是什么？**
>
> 语言模型并不能直接处理词元，它们处理的是词元对应的词嵌入。一个词嵌入就是一串数字，比如说“你”这个词元可能对于的词嵌入是 [0.1, 0.2, 0.15, ......]，这串数字的长度是人为设定的，一般来说，词嵌入的长度以千计。
>
> 大模型在训练过程中也会不断更新词嵌入，这些词嵌入相当于是大模型对某个词元的理解，虽然人不能解释这串数字的含义。 每个词元的词嵌入一开始是随机数，只有模型开始学习了之后，词嵌入不断更新，才最终变成了大模型对某个词元的理解。
>
> 整个流程的大意如下图，上半部分是流程示意，下半部分是例子。最后的双箭头指的是：前向，词嵌入输入模型，然后也可以反向更新词嵌入。
>
> ![tokenization_embedding](./tokenization_embedding.png)

但是，可能大模型训练用的语料跟迭代分词器用的语料不一样。这样的话，大模型虽然有“SolidGoldMagikarp”的词嵌入，但是它训练的时候从来没有见过这个词，所以这个词的词嵌入从来没有更新过，一直是一开始的随机数。
大模型不能理解这些词嵌入（随机数），在理解的盲区里，所以遇上这些词之后就出现奇怪的行为了。

在这个例子里，我们仔细审视，至少有三个地方是（或者可以）人为设定的：

1. 什么字符串作为一个词元？
    * 例如我们可以人为添加 “<\|endoftext\|>” 这个字符串作为一个词元
2. 分词器的设置、迭代数据和迭代策略
    * Andrej 提到的一个例子是：Sentencepiece 迭代分词器的时候可以设置把罕见词替换成 “\<unk\>” 这个词元，“unk” 是 “unknown” 的缩写，意思是未知字符。
    * 至于迭代数据，假如说我们迭代分词器用的数据就不包含中文，那么注定这个分词器对中文的处理不好。
    * 另外，分词器怎么处理数字，会直接影响到 LLM 的数学能力。
3. 用于 LLM 训练的数据：一些用来跑分的英语大模型，从一开始训练的时候就去掉了非英语的数据，这样虽然可以节省成本，但是注定模型不会理解英语之外的其他语言，即使它本身足够智能。

> **思考题一：大模型训练里的歧视？**
>
> 如果一个产品的说明书只有英文，如果说明书作者说：我们只考虑英语用户，你会不会觉得被歧视了？
> 同样的，当你在 LLM 的训练报告或者论文里看到类似于“我们剔除了数据集/语料中的非英文部分”，你会不会觉得被歧视了？
>
> 还有更微妙的，假如分词器迭代语料里有99%是英文，然后分词器迭代设置是：开启罕见词过滤、常见词覆盖率是 99%，意味着出现少于 1% 的词都会被替换为 “\<unk\>”，你会不会觉得被歧视了？
>
> **思考题二：AGI 到来了吗？**

这个例子能拓展的话题太多了，但是我们先回归正题，也就是这个例子跟 AI 第一性原理有什么关系。 LLM 的例子里，数据的体量和质量的重要性不需要我多说了，我们需要联想的是第三条，也就是
> 信息的流动应该减少人为设计，让模型自己学习，尽可能保留对模型有用的信息

假如说我们能够去掉分词器，每一个字节就是一个词元，那么 Andrej 提到的十一个问题里的大部分就可以迎刃而解了。这些问题之所以存在，还是在于分词这个步骤，以及这个步骤里人工设计的部分（heuristics）。我可以说这甚至影响到了 AGI 的到来。

在这个例子里，系统指的是分词迭代数据、分词器、LLM 训练数据和 LLM 的组合。我们从表象上，也就是 Andrej 提到的十一个问题，追溯到了工程上的根源，也就是分词器。但是思维上追根溯源，本质在哪里？我觉得还是人为的设计。人类觉得对世界的理解是正确而且有效的，而且很多时候我们觉得世界是可以理解的，是有秩序的，所以很多时候会把我们的思维融入到我们的创造里。
但是其实我们都不完全了解我们自己的思维，更别说完全理解世界了。甚至就连围棋，我们对它的理解，可能都没有从零训练、不看棋谱的 AlphaZero 要深刻。而且其实我们的思维很大部分是“混乱” （见《快思慢想》，写得很好）。

虽然算不上无数，但是很多例子已经告诉我们，理解世界的方式有很多。而 Rich Sutton 写的《The Bitter Lesson （苦涩的教训）》也总结了类似的结论，也就是 AI 理解世界的方式不一定要符合人类的设计，我们需要减少人为的设计才可能训练出更强大的 AI。

> **但是等一下，我们能不能设计出完美的分词器来解决这些问题呢？**
>
> 这个问题其实跟 NLP 历史上的一个问题类似，就是“我们能够用规则来建模、理解语言吗”。而语言是我们思维的一种体现，其实问题也可以转换成“我们能够用规则来建模、理解我们的思维吗”。
> 我的答案是“小部分可以，例如数学，但是大部分是不能用规则来框定、理解的”，那翻译过来就是“在一些特定应用上，你可以试着设计完美的分词器，但是通用的完美分词器是不存在的”。
>

另一个例子可能更容易理解，也更贴近应用，但是我觉得却是更加复杂的，就是 LLM 语音对话。
这个也是我实际遇到的问题，就是我们怎么让人可以和 LLM 通过语音对话。对话是双向的，也就意味着人能说话，LLM 也要能说话。
如果你比较熟悉现有的 AI 组件的话，那第一时间想到的可能是：（这还不简单？）用 ASR 把人的话转换成文字，文字发给 LLM，然后用 TTS 把 LLM 的文字转换成声音。

一开始我也是这么想的。首先，我们怎么知道用户想和 LLM 对话？算了，我做的还是概念验证，就先假设用户会按下按钮和 LLM 对话吧，毕竟 ChatGPT 也差不多是这样做的🐶
然后就到了语音识别，ASR 的准确率还行，有的词会听错，那么我应该加热词/常见词就可以解决了（吧？）；
LLM 问题不大，我用 [Dify](https://dify.ai) 来做各种提示词工程，而且只要 ASR 的结果没有错得离谱，LLM 也有一定根据上下文纠错的能力；
然后就是文字转语音，TTS 很多云厂商都提供了，随便调用一个就行，就是把 LLM 的输出发送到服务器嘛。那么整个流程的概念图像是这样

![speech_conversation_concept](./speech_conversation_concept.png)

看到这个图，估计有的人从第一个例子的教训里联想，就开始感觉有点不对劲了。但是我当时没觉得有什么问题，这就像是标准答案。我们先忍住不对劲的感受，暂且当它是标答的思路，把它实现出来。下面是我第一版的工程实现的流程图。

![speech_conversation_implementation_v1](./speech_conversation_impl_v1.png)

遇上的第一个工程问题是延迟。这个实现是最简单的，只是延迟会有十多秒。就算是在 XP 时代，十多秒的延迟也会让用户杀掉进程了。但是很明显，工程实现还可以改进，所以我还可以修修补补，就像 Andrej 教程里的第一版的分词器。

![speech_conversation_implementation_final](./speech_conversation_impl_final.png)

新三年，旧三年，缝缝补补又三年。总之上图是最终的实现，把能流式传输的地方全都做了流式。这样，用户感知到的延迟就是结束说话到第一个音频块播放之间的时间，大概可以做到 2-4 秒。后续我还可以把 LLM 换成微调过的小模型，那么延迟会进一步降低。

工程部分点到即止，这个例子跟 AI 第一性原理有什么关系？我绕一个稍微远一点的路，但是值得。

绕路的地方在于，我们看终版实现里，有很多“块”的概念，“音频块”“文字块”“语音块”。我们回想一下词元（token）是什么，其实它也是“块”，是字符串的一块。LLM 的输入是按词元，也就是一块一块输入的，而它的输出也是一块一块的。
既然都是一块一块的，我们能不能把“音频块”“文字块”“语音块”和“词元”都统一起来，这些统一的块作为输入给 LLM，然后它也可以输出这些块。其实英语文献里，这些都是 "token"，只能说研究者懒得起名了。一个例子是[通义千问 Audio](https://www.modelscope.cn/models/qwen/Qwen-Audio/summary)，深度学习从业者可以看看它的实现。
只不过通义千问 Audio 虽然可以接收文字块和音频块，但是只能输出文字块，也就是词元。

> 拓展：GPT-4 Vision 会把图片切块，变成一片一片的图片块，然后对图片块做嵌入（embedding），然后输入到模型骨干里。

但是终版实现和统一块的区别是什么？最核心的区别在于数据的表征（Representation）和模型的设计。我们看终版实现的这个语音对话系统，包含了三个部分以及相应人为设计的地方：

| 组件  |             人为设计（heuristics）              |
|:---:|:-----------------------------------------:|
| ASR | ASR 模型的训练数据、ASR 模型的分词器、ASR 模型的推理设置（如热词）等  |
| LLM |           像上文讨论的，分词器、语料和各种推理设置            |
| TTS | TTS 模型的训练数据、TTS 模型的分词器、TTS 的推理设置（如音色、语速）等 |

在这个系统里，每个组件之间的交互数据的表征是文字。ASR 给 LLM 的是文字，LLM 给 TTS 的是文字。
不讨论别的人为设计，光是分词器，这个系统里就有三个不同的。

![speech_conversation_tokenizers](./speech_conversation_tokenizers.png)
> 一个分词器在 ASR 的输出部分，一个在 LLM 的输入和输出部分，还有一个在 TTS 的输入部分。

根据第一个例子，我们不难推出，分词器会造成信息的损失。例如，分词器把“罕见”的词元直接替换成了“\<unk\>”，词元的意思就完全灭失了。
信息的损失不止于此，使用这个系统，用户不能说“你听，这是什么声音？”或者“你能不能学猫叫？”。在这个系统里，这些限制不是因为 LLM 本身智能不够，而是我们人为添加的组件的限制。
LLM 不能辨别声音，是因为我们的 ASR 只训练成只能分辨人说话的内容；LLM 不能模仿动物声音，是因为 TTS 只能发出人的声音。

就算我们训练出来万能的 ASR 和 TTS，可以识别、发出各种声音，因为这个系统里交互数据的表征是文字，也会造成不必要的信息损失。
一个直白的例子就是，我作为一个南方人，从来没有见过雪。虽然物理书上写过雪的结构，文学上描写雪的文章、诗句数不胜数，但是这些都比不上我去到北方，亲眼看到雪，用手捧起一堆雪。有很多事物、概念、感受和微妙之处是文字描述不出来的。

说得有点远了，另一个真实的例子是图像理解。在 GPT-3.5 发布之后，GPT-4 发布之前，很多研究者想让 GPT-3.5 “理解”图片。一种**真实的**做法就是，使用不同的 CV 模型（比如说目标检测、语义分割、实例分割）把图像的信息尽可能完整抽取出来。这些信息包括但不限于：

* 图像中有什么种类的物体，例如“图中有狗和猫”
* 图像中有多少个物体，例如“图中有 1 只猫和 3 只狗”
* 图像中有多少个不一样的实例，比如说“图中有汪汪（狗）和咪咪（猫）”
* 图像中实例的相对位置，比如说“汪汪在咪咪的旁边，右边”
* ······你可以想各种各样的规则来描述一张图

但是 GPT-4 发布之后，人们发现它不仅仅可以做到上面的事，还能做更多，例如：根据射门照片，预测会不会进球；分析图中人物的微表情和情绪等等。GPT-4 能帮用户创造的价值当然也多得多。
而我们这个 LLM 语音对话系统里的（ASR + LLM）部分，跟（CV 模型 + GPT-3.5）的组合是惊人地相似。用深度学习的术语来说，这两种组合都不是端到端的（End to End）。

> 《咒术回战》的粉丝可以想一下，假如与幸吉操纵机械的方式不是通过天赋直接和机器相连，而是通过发消息，会是什么样子 😂
>
> 机械：老大，我听到 11 点方位有声音，很微妙，像是猫的脚步
>
> 与幸吉：好，去看看，如果是猫，摸摸它，要温柔一点；不是的话小心有埋伏！
>
> 机械：老大，是猫！银渐层！看起来像是母猫，毛很有光泽，像是用了飘柔！
>
> 与幸吉：摸摸摸摸，凶不凶？

我们可以想象一下，假如说一个 LLM 就是用文字和音频训练的，能够同时理解文字音频，也能生成文字、音频回复，那么我们的这个语音对话系统在架构上就可以变得非常简洁。

![audio_llm_flow](./audio_llm_flow.png)

结合 GPT-4 Vision 的视觉能力，我们稍微扩展一下 LLM 的结构，那么就可以得到多模态大模型的结构，如下：

![multimodal_model_architecture](./multimodal_model_architecture.png)

浅绿色的部分是当前所有 LLM 都有的组件；浅蓝色的部分是理解、生成音频能力所需要的组件；浅黄色是 GPT-4 Vision 用来理解图片的组件；而浅红色部分是大模型生成图片需要的组件。
如果你对大模型关注得比较多，这张图应该非常熟悉了。这种架构其实也是学术界、工业界发展多模态大模型的共识，也就是保留 LLM 的骨干，不断添加各个模态的编码器、解码器，让模型去理解、生成各种模态的数据。一个最新的例子是谷歌的 Gemini，里面就包含了音频、视频、图片的编解码器。

> 注意这里的编码器、解码器和传统多媒体处理里的编码器、解码器的实现是完全不同的。这里的编解码器是神经网络，编码器把数据编码成嵌入向量（embedding），解码器则是把向量转换为数据。

所以这个例子的意义是什么呢？我觉得还是留给你们读者来总结。在这里，你们可以停一下，喝口咖啡、洗个澡，或者睡个觉，都可以。*Just let that sink in.*

![let_that_sink_in](./sink_in.png)

因为，下面我们要进入一个副本章节，一个令人激动的问题——

#### OpenAI 的边界在哪里？

这个问题的答案可以很简洁——“AGI 需要的能力，OpenAI 都会做”。但是这个答案没什么指导意义，不过我们可以慢慢拆分，拆成更细的问题，比如说：

* AGI 需要的能力有什么？
* OpenAI 会做到哪种程度？

我们先用一个例子来入门，再用另一个例子来发散思考。

第一个例子是一个问题 “Sora 的下一步是什么？” 我觉得下一步应该是把模型的各个组件、训练的方法和数据整合到 GPT 中。
就像上文说的，目前 GPT-4 能看见、理解图像信息，但是不能创造。创造是人的一种能力，那么 AGI 也要有。至于“Sora 是世界模拟器”或者通过 Sora 构建世界模型，其实是 AGI 愿景的一小部分，是手段不是目的。OpenAI 的愿景只有一个，就是实现 AGI。
愿景归愿景，但是 Sora 的下一步和 AI 第一性原理有什么关系？我们从 Sora 目前能够支持的输入、输出来分析。

![sora_input_output](./sora_io.png)

它的输入和输出的模态是不对称的，但是这有什么问题呢？信息流通的问题，具体来说是沟通的问题。现在的 Sora 很厉害没错，但是我觉得还没到一部分人说的**颠覆**影视行业的地步。我自己就在影视行业里做 AIGC，根据我的采访和观察，行业里的创作者的很大一部分精力花费在沟通和协作上。
例如，导演不仅仅要给出文字分镜，要给出画面分镜，甚至有的时候要自己做一段视频来示意，为的是摄影团队能够准确理解导演的想法并且准确执行。执行的时候出现了偏差，团队需要不断沟通来解决问题。但是目前 Sora 做不了沟通，创作者只能希望提供的信息能够被 Sora 准确理解，然后生成图片或者视频。
想象你是导演，你要用 Sora 拍片。Sora 生成的第一版视频里的场景和人物就足够惊艳了，但是你觉得相机的轨迹或者机位需要调整，这个时候你要怎么和 Sora 沟通？做不到。

或许你也听过类似于“精准控制”这样的要求，但是往深一步想，这是沟通的问题，就是用户能不能和 AI 不断沟通来解决问题。而我们再往 AI 第一性原理的方向再想一步，沟通的问题是信息流通的问题。
要彻底解决沟通问题、信息流通的问题，Sora 就不能只是渐进式地改进（比如加入 ControlNet，添加 LoRA 支持等），而是把 Sora 放进一个更加通用的系统，也就是 GPT 里。

想象一下，当 GPT 可以像一个画家、摄影师、导演一样和你沟通。沟通的时候它可以最完整地理解你给它的文字、音乐、视频、图片，并且在沟通中不断修改它的创造物（图片、视频、声音等）。这个时候，影视行业才会被颠覆、革命。

> Film and television industry is nothing without its people *and* communication!

OpenAI 会让 GPT 有多模态创造的能力，但是就像是 Sony 作为摄像机制造商不会关心用户拍什么类型的电影，OpenAI 不会关心用户和 GPT 创造的内容（除非是为了训练），这也是 Sam Altman 在 YC-2024届上讲 “以存在一个 AGI 为前提来创业” 的含义。

当我们以 AI 第一性原理和 “以存在一个 AGI 为前提来创业” 为原则审视目前的 AI 独角兽的时候，有一个我觉得有必要来讨论一下。这就是第二个例子 —— [Elevenlabs](https://elevenlabs.io)

Elevenlabs 的技术 （他们称之为 *Generative Voice AI*）就主要集中在语音，例如说 TTS 和语音转语音（Speech to Speech）。TTS 不用再赘述了，而语音转语音的功能包括：

* 保留音色，转换内容的语言。例如让 Taylor Swift 讲中文。
* 保留内容，转换音色。例如转性。

我用过 Elevenlabs 的 TTS，生成音频的质量确实很好，比别的竞品都好很多，但是我觉得它会不幸地变成下一个 OpenAI 降维打击的牺牲者。
无论以 AI 第一性原理还是 “AGI 需要的能力，OpenAI 都会做” 这个原则，GPT 都最终会获得理解、创造音频的能力。而 TTS 和语音转语音的功能仅仅是理解、创造音频的能力的一部分而已。
到那时，我们甚至可以让 GPT 解读我们的狗狗在说什么，然后想象一个合适的音色，让我们的狗狗以各种语言跟我们打招呼。到那时，Elevenlabs 的护城河在哪里呢？

通过这两个例子和基于 AI 第一性原理的分析，我觉得很多人会对“OpenAI 的边界在哪里”这个问题有更多实感。

> **传统的 TTS 和 ASR 就一定会灰飞烟灭吗？**
>
> 在一开始写完这个章节的时候，我很自信，觉得答案是肯定的。但是在仔细审视了上面的 LLM 语音对话的例子之后，我觉得传统的 TTS 和 ASR 厂商想要继续活下去，就要做到极致地细分场景。
> 其实历史上也有例子，就是传统的 CV。理论上说，只要 GPT-4-Vision 足够智能，传统的 CV 的用例（例如目标检测、人脸识别等）都会被降维打击，但是现在还没到颠覆的时候，就是因为传统的 CV 做了足够多的细分场景优化。
>
> 而传统的 TTS 和 ASR 的细分场景优化在哪？我跟我的一位同事也是朋友交流之后得到的结论是：它们最终都会走向极致的本地化和定制化。但是，后果就是，赚钱难赚了，最终都是辛苦钱。
>
> 举个例子，假设 GPT-5 能理解、生成各种声音，理论上 TTS 和 ASR 就完全没有意义了。但是落地时可能的问题是，GPT-5 能够识别山东烟台方言吗？GPT-5 能够说烟台方言吗？很可能不行，但是在实际落地实用的时候要面对的问题就可能是细化到这种程度。
> 即使 GPT-5 足够聪明，能够通过上下文学习（In-context Learning）的方式来快速学习，但是它的推理速度足够快吗？这个也是关键问题。
>
> 只有极致细分的优化才能解决上面这些落地问题。

> 给《咒术回战》的粉丝：与幸吉操纵机械丸的时候，会用 TTS 说话吗 😂

### AI 第一性原理对产品设计的意义

上述的例子都是 AI 第一性原理的体现。为了尽可能保留信息的体量、质量和流通效率，我们要尽可能把各种能力嵌入到模型里，而不是把各种能力分散到各个组件里。
这样的设计是为了尽可能减少人为设计，让模型自己从数据中学习，尽可能保留对模型有用的信息。但是我们怎么知道要把哪些能力嵌入到模型里呢？也就是说，我们希望模型实现什么功能？满足什么需求？这就是产品设计的问题了。

就算不管上面的问题，AI 模型的开发、训练过程就非常像开发一个产品：

1. 首先要定义解决的问题是什么？更具体地，这个问题能不能量化？有没有额外的指标要满足？
2. 然后调研现有的模型，看有没有合适的。如果没有，需要重新设计；如果有，则看有什么地方可以改进。
3. 定义好场景和用例。更具体地是定义好数据集和评价标准。
4. 训练模型，不断迭代、评估，直到能够交付。
5. 交付之后还要测试真实场景里的表现，看是不是真正地解决了问题，或者性能有没有偏差。

当我们用产品思维去看 AI 模型开发的时候，我们反而可以对 AI 技术有更加深刻的理解。
就像开发产品，我们需要找准定位和需求，开发 AI 模型也要清晰定义问题；就像开发产品会面对不确定性，AI 模型的开发也是充满不确定性的；就像开发产品最终需要用户验证，AI 模型的表现也需要不断面向用户验证。

当我们在产品设计上决定好模型的能力范围，并且把这些能力在工程上直接嵌入到模型里，训练、迭代直至变成模型的一部分，那其实模型也就变成了产品。
OpenAI 2023 DevDay 的分享就让我很有启发，让我看到了 LLM 背后大量的产品研究。分享的题目是 [Research × Product](https://youtu.be/YXiRbRacTF0?si=AMPRRhnSVhTnn_1l)。

我在这里只摘取一个例子。这个例子是一个简单又复杂的问题——“当用户说’你好’的时候，LLM 要怎么回复？” 相信你可以想出一百种回复，但是这些回复有什么不同呢？哪一些会更好？怎么样才算“更好”？如果你经常使用 ChatGPT，那估计第一个在你脑子里冒出来的回答会类似于

> 你好，今天我可以怎么帮你？
>
> Hello! How can I assist you today?

这个回答看起来平平无奇，但是仔细想想，这一句话准确定义了 GPT 的角色，也就是 "assistant"。如果换成别的，例如说“你好！今天过得怎么样？”，那 GPT 的角色就从助手变成类似于朋友了。
实际上，这种影响远比很多人想象得深刻。当我通过提示词，想让 GPT 变得更加像朋友的时候，我不得不在提示词里写了三遍类似的意思：“你不是用户的助手，而是朋友”“你不要说类似于‘我怎么帮你’这样的话”“你和用户是平等的，不是从属的关系，你不需要帮助 TA，TA 也不需要你的帮助”。
ChatGPT 的助手定位对 LLM 的影响已经渗透到了它的回答、思考的方方面面。我甚至觉得 OpenAI 都可以把 "How can I assist you today" 作为 ChatGPT 的产品 Slogan 并且注册商标了。

我们在别的地方也看到模型即产品这种模式涌现出来。一个非常突出的例子是各种定制化 Stable Diffusion (SD) 模型和 SD 的 LoRA 模型。很多人在 [CivitAI](http://civitai.com) 或者 [Liblib](https://liblib.art) 上发布他们训练的模型。有的是免费分享，有的是要付费授权的。
用户作为 SD 模型的产品经理，定义的问题是他们的模型可以生成什么样的美学风格的图片，或者有哪些图像处理的能力。

模型可以变成产品，产品需求也可以推动模型开发，这就变成了产品驱动的 AI 模型开发。
一个例子是 ControlNet。因为原本的 SD 只能靠文字指引图片生成，但是实际的需求需要更多，比如根据草图来生成完善后的图片。这个时候就不得不倒逼模型的开发了。
而另一个非常好的例子是妙鸭相机。这个非常有名，并且有非常多分析的文章，就不赘述了。

当模型的能力可以解决特定问题的时候，模型就变成了产品。；反过来，如果现有的模型能力不足以解决某个问题，那也可以从需求出发，开发新的模型，把能力嵌入到模型里。
假如这个问题足够有价值或者足够广泛，根据 AI 第一性原理，不使用现有的 AI 组件拼凑、整合，把能力嵌入模型能够实现的能力上限会更高，并且创造更多价值。

虽然我不是硬件工程师，但我非常喜欢苹果招聘硬件工程师的 Slogan —— “别人眼中的终点，正是我们最喜欢的起点”。这句话深深呼应了 Alan Kay 的名言 —— “对软件较真的人，应该做自己的硬件”。而我觉得在今天这个 AI 时代，这两句话可以有新的进化

> 对用户体验较真的开发者，应该做自己的 AI 模型。

做 AI 模型不是为了做而做，不是跟风，而是在 AI 第一性原理的背景下，根本性提升用户体验的必经之路。
就像上面的 LLM 语音对话的例子，我优化代码到最后，发现如果还要进一步提升用户体验，模型本身的改进是绕不过的。这种改变的代价肯定会是另一个量级，但是我们也可以发现另一个量级的新机会。

上面这句话的“开发者”不仅仅是软件开发者，我觉得还应该包括消费电子的硬件开发者和设计师。但是 AI 第一性原理对硬件产品设计的意义是什么？我不是硬件工程师或者设计师，这里我举一个例子，谈谈我自己的看法。

[Rabbit R1](https://www.rabbit.tech/rabbit-r1) 最近在科技圈里非常火热，争议也很多。一开始，单单看表面上的交互，我的印象是偏负面的。但是看完他们 [Research](https://www.rabbit.tech/research) 的介绍之后，我觉得他们很有可能走在一条新的道路上。这条道路，和 AI
第一性原理在大方向上是一致的。因为技术上的细节他们透露得不多也比较模糊，所以我只能做一些猜测，基于这些假设（斜体）来给出我的看法。这也就意味着我的看法只有在假设正确的前提下才有意义（叠甲）。

Rabbit R1 的主要交互方式是说话，对着它说话，然后 AI 会分析用户的意图，替用户做出各种动作。这也是 Rabbit 团队把他们的模型称之为动作模型（Large Action Model - **LAM**）的原因。

假如说*在 LAM 系统里，语音模块还是使用 ASR 和 TTS 实现的*，那么类比我们在 LLM 语音对话的例子里的分析，不难得出一个结论：这个系统的上限不会太高。它最多是一台更好更智能的锤子
TNT。而锤子 TNT 在 ASR 遇上的问题，Rabbit R1 同样也有，例如说噪音下识别困难的问题。

![锤子 TNT](./Smartisan_TNT.png)
> 锤子 TNT 在 AI 应用的意义上，领先了业界 5 年，可谓生不逢时

![Rabbit R1 表情包](./rabbit_r1_meme.png)

它只能听懂人的语音，理解不了别的声音，也就意味着在输入信息量方面，Rabbit R1 和 ChatGPT 手机 App 是一样的。当 ChatGPT 手机 App 的模型换成了能够理解声音的新一代 GPT，在各种基于声音的新用例涌现，Rabbit R1 在理解声音方面就变成了功能机，没人会在意 Rabbit R1 是否比手机更加轻巧便捷。

而在输出信息量方面，Rabbit R1 有语音输出或者可以在小屏幕上显示。在输入和输出信息量这方面，我只能说不如手机。需要输入的时候，同样是举起一个设备，举起手机可以拍照、打字、说话，但是举起 Rabbit R1 只能拍照或者说话；查看输出的时候，手机和 R1 同样可以听语音输出，但是手机还能更好地显示文字和图像。
所以，一些人对 Rabbit R1 硬件的批判是有道理的。在硬件上，我始终觉得智能眼镜会是 AI 最好的载体。在智能眼镜上，AI 能看到你看到的，听到你听到的，AI 能让你看见，AI 也能让你听见，而且这些都不用你举起一个设备。

Rabbit 的产品就一无是处了吗？我觉得也不是，至少 Rabbit 团队的想法走在了前面。在 [Rabbit Research](https://www.rabbit.tech/research) 页面的介绍里，他们确实在训练、微调着自己的模型，把越来越多的能力融合进 LAM 本身，在模型的层面尝试解决应用的问题。这个方向是和 AI 第一性原理相符合的。
当 LAM 具备越来越多的能力，没准 Rabbit R2 就会是一副 AI 附身的智能眼镜呢？那会是革命性的！



TODOs:

* 梯度 API 和 AI OS
* AI 原生硬件的想象

## 总结和亮点

TODO

## 后记

TODO

## 元数据

版本：0.0.4

日期：2024-02-19

版权协议：[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)

### 更新日志

2024.02.25: 写完了 AI 第一性原理的基本概念和两个例子

2024.02.26: 写完了 “OpenAI 的边界在哪里” 章节